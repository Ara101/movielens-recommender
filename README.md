# MovieLens Recommender

A from-scratch SVD collaborative filtering recommender system on MovieLens 100k, built entirely in PyTorch. Includes training, evaluation with ranking metrics (NDCG, Precision, Recall, HitRate), top-N recommendation generation, and performance visualisation.

## Performance

| Metric | Value |
|--------|-------|
| **RMSE** | 1.08 |
| **MAE** | 0.90 |
| **NDCG@10** | 0.15 |
| **Precision@10** | 0.12 |
| **Recall@10** | 0.10 |
| **HitRate@10** | 0.55 |

Evaluated on a 80/20 random split of MovieLens 100k (943 users, 1,682 movies, 100k ratings). Relevance threshold: rating >= 4.

---

## Quick Start

### Prerequisites

- Python 3.10+ (tested on 3.11)
- pip

### 1. Clone and Install

```bash
git clone https://github.com/<your-username>/movielens-recommender.git
cd movielens-recommender
pip install -r requirements.txt
```

The `requirements.txt` installs:
- **PyTorch** — model implementation and training
- **NumPy** — data processing and metrics
- **Matplotlib** — plotting
- **Pytest** — unit tests
- **Jupyter** — notebook exploration

### 2. Train the Model

```bash
python src/train.py
```

Trains the SVD model on MovieLens 100k and saves weights to `models/svd_rating_predictor.pt`. Takes ~5 seconds on CPU.

### 3. Run the Full Showcase

```bash
python src/showcase.py --user 42
```

This produces a complete report: model summary, metrics, user profile, and top-10 recommendations with movie titles, plus a 4-panel figure saved to `results/showcase_report.png`.

---

## Project Structure

```
movielens-recommender/
├── src/
│   ├── train.py           # Train the SVD model and save weights
│   ├── evaluate.py        # Compute RMSE, MAE, and ranking metrics
│   ├── recommend.py       # Generate top-N recommendations for a user
│   ├── compute_ndcg.py    # NDCG, Precision, Recall, HitRate functions
│   ├── plot.py            # Generate performance visualisation plots
│   ├── showcase.py        # End-to-end demo tying everything together
│   ├── test.py            # Unit tests (29 tests)
│   └── utils.py           # Shared utility functions
├── models/
│   ├── svd_rating_predictor.py   # SVD model definition (nn.Module)
│   └── svd_rating_predictor.pt   # Trained model weights
├── results/               # Metrics + plots (generated by scripts)
├── data/ml-100k/          # MovieLens 100k dataset
├── notebooks/eda.ipynb    # Exploratory data analysis
├── requirements.txt       # Python dependencies
├── cheat-sheet.md         # Quick reference / interview notes
└── README.md
```

---

## Walkthrough: Every Script Explained

### `models/svd_rating_predictor.py` — The Model

The core model. Implements **Funk SVD** (the algorithm behind the Netflix Prize winner) from scratch in PyTorch.

**Prediction formula:**

```
r̂(u, i) = μ + b_u + b_i + p_u · q_i
```

- `μ` — global mean rating (3.53 for MovieLens 100k)
- `b_u` — per-user bias (does this user rate high/low?)
- `b_i` — per-item bias (is this movie generally liked?)
- `p_u · q_i` — dot product of learned latent factor vectors

**Key classes:**
- `SVDRatingPredictor(nn.Module)` — single model with `forward(user_ids, item_ids)` and `predict_all_items(user_id)` for ranking
- `EnsembleSVDPredictor` — wraps N models, returns mean prediction + uncertainty

**Parameters:** ~55k (20 latent factors × 943 users + 1,682 items, plus biases)

```python
from svd_rating_predictor import SVDRatingPredictor

model = SVDRatingPredictor(num_users=943, num_items=1682, n_factors=20)
```

---

### `src/train.py` — Training

Trains the SVD model on MovieLens 100k ratings.

```bash
python src/train.py
```

**What it does:**
1. Loads ratings from `data/ml-100k/ml-100k/u.data` (tab-separated, 100k lines)
2. Converts to 0-indexed `(user, item, rating)` triples
3. Splits 80/20 with `seed=42` for reproducibility
4. Trains the `SVDRatingPredictor` with Adam optimiser (lr=0.005, L2 reg=0.02)
5. Mini-batch SGD, 50 epochs, batch size 1024
6. Prints train loss + test RMSE every 10 epochs
7. Saves trained weights to `models/svd_rating_predictor.pt`

**Key functions:**
- `load_ratings(path)` — parse `u.data`, returns `(triples, num_users, num_items)`
- `train_test_split_ratings(ratings, test_ratio, seed)` — deterministic split
- `train_svd_predictor(train, test, ...)` — full training loop, returns `(model, rmse)`

**Example output:**
```
TRAINING SVD RATING PREDICTOR (from scratch)
Global mean: 3.5319
Train ratings: 80,000  |  Test ratings: 20,000
Model params:  55,125
Epoch  10/50  |  Train Loss: 1.3044  |  Test RMSE: 1.1098
Epoch  50/50  |  Train Loss: 0.9271  |  Test RMSE: 1.0791
Final Test RMSE: 1.0791
```

---

### `src/evaluate.py` — Evaluation

Computes both rating-prediction and ranking metrics on the test set.

```bash
python src/evaluate.py
```

**What it does:**
1. Loads the trained model from `models/svd_rating_predictor.pt`
2. Computes **RMSE** and **MAE** on 20k test ratings
3. For each test user, generates a top-10 ranked list from unrated items
4. Computes **NDCG@10**, **Precision@10**, **Recall@10**, **HitRate@10**
5. Saves all metrics to `results/metrics.txt`

**Key functions:**
- `compute_rmse(model, test_triples)` — rating prediction error
- `compute_mae(model, test_triples)` — absolute error
- `save_metrics(results, filepath, k)` — persist to file
- Uses ranking functions from `compute_ndcg.py`

---

### `src/recommend.py` — Recommendations

Generates personalised movie recommendations with titles and genres.

```bash
python src/recommend.py --user 42 --n 10
```

**What it does:**
1. Loads trained model + MovieLens item metadata (`u.item`, `u.user`)
2. Shows the user's profile: age, gender, occupation, top-rated movies
3. Predicts ratings for all unrated items, returns top-N
4. Prints recommendations with movie titles and genres

**Key functions:**
- `load_item_metadata()` — movie titles + genres from `u.item`
- `load_user_info()` — demographics from `u.user`
- `get_top_n_recommendations(model, user_id, rated_items, num_items, n)` — ranked list
- `get_user_history(user_id, ratings)` — what the user rated

**Example output:**
```
  User 42  |  Age: 30  |  Gender: M  |  Occupation: administrator
  Rated 183 movies  |  Avg rating: 3.73

  Top-5 Recommendations:
  Rank  Pred   Title                                      Genres
  1     3.86   Star Wars (1977)                           Action, Adventure
  2     3.76   Fargo (1996)                               Crime, Drama, Thriller
  3     3.75   Shawshank Redemption, The (1994)           Drama
```

---

### `src/compute_ndcg.py` — Ranking Metrics

Core implementation of all ranking evaluation metrics.

```bash
python src/compute_ndcg.py
```

**What it does (standalone):**
1. Loads trained model + data
2. Evaluates NDCG, Precision, Recall, HitRate at k = 1, 3, 5, 10, 20, 50
3. Prints a comparison table and saves to `results/metrics.txt`

**Key functions (also imported by other scripts):**
- `ndcg_at_k(relevant_items, recommended_items, k)` — Normalized DCG (binary relevance)
- `precision_at_k(relevant_items, recommended_items, k)` — fraction of top-k that are relevant
- `recall_at_k(relevant_items, recommended_items, k)` — fraction of relevant items found
- `hit_rate_at_k(relevant_items, recommended_items, k)` — at least one hit in top-k?
- `evaluate_all_users(model, train, test, num_items, k)` — aggregate metrics across users

**Metric definitions:**

| Metric | Formula | Intuition |
|--------|---------|-----------|
| NDCG@k | DCG / IDCG | Rewards relevant items ranked higher (log discount) |
| Precision@k | hits / k | What fraction of recs are good? |
| Recall@k | hits / total relevant | What fraction of good items did we find? |
| HitRate@k | 1 if any hit else 0 | Did we get at least one right? |

---

### `src/plot.py` — Visualisations

Generates 6 performance plots and saves them to `results/`.

```bash
python src/plot.py
```

**Plots generated:**

| File | Description |
|------|-------------|
| `training_curve.png` | Train vs test RMSE over 50 epochs |
| `predicted_vs_actual.png` | Scatter plot of predictions vs ground truth |
| `rating_distributions.png` | Actual vs predicted rating histograms |
| `metrics_at_k.png` | NDCG@k and Precision@k for varying k |
| `per_user_error.png` | Distribution of per-user MAE |
| `top10_recommendations.png` | Bar chart of top-10 recs for a sample user |

---

### `src/showcase.py` — Full Demo

The main showcase script. Ties together train, evaluate, recommend, and plot into one polished output.

```bash
python src/showcase.py --user 1          # default
python src/showcase.py --user 42 --n 5   # user 42, top 5
```

**What it produces:**
- **Console:** model summary, RMSE/MAE, ranking metrics table at k=1/5/10/20, user profile with favourite movies, top-N recommendations with titles and genres
- **`results/showcase_report.png`** — 4-panel figure (predicted vs actual, metrics@k, top-N bar chart, rating distributions)
- **`results/metrics.txt`** — all numeric metrics

---

### `src/test.py` — Unit Tests

29 tests covering data loading, model architecture, metrics, training, and end-to-end ranking.

```bash
python -m pytest src/test.py -v
```

**Test classes:**

| Class | Tests | What it covers |
|-------|-------|----------------|
| `TestDataLoading` | 8 | Rating count (100k), dimensions (943×1682), 0-indexing, valid ranges, split sizes, determinism |
| `TestSVDModel` | 7 | Output shape, clamping [1,5], `predict_all_items`, param count, global mean buffer, gradient flow, save/load |
| `TestEnsembleModel` | 3 | Model count, output shapes, non-negative uncertainty |
| `TestMetrics` | 8 | NDCG perfect/worst/empty/partial/bounded, Precision perfect/none/partial |
| `TestTraining` | 2 | RMSE < 3 on synthetic data, RMSE < 2 on real MovieLens (5 epochs) |
| `TestEndToEndRanking` | 1 | Full pipeline: load model → predict all items → verify top-10 sorted |

---

### `src/utils.py` — Utilities

Shared helper functions used by other scripts.

- `load_saved_model(path)` — load trained SVD model from `.pt` file
- `get_user_rating_history(user_id)` — all ratings for a user
- `get_model_info(model)` — parameter count, factor count, global mean

---

## How the Algorithm Works

**Funk SVD** decomposes the user-item rating matrix into low-rank factors:

```
R ≈ μ + B_users + B_items + P × Q^T
```

Each user `u` is represented by a bias `b_u` and a latent vector `p_u ∈ ℝ^20`.
Each item `i` is represented by a bias `b_i` and a latent vector `q_i ∈ ℝ^20`.

The predicted rating is:

```
r̂(u, i) = μ + b_u + b_i + p_u · q_i
```

Training minimises MSE with L2 regularisation using Adam. The model is trained on 80k ratings and evaluated on 20k held-out ratings.

**For ranking**, we compute `r̂(u, i)` for all unrated items and sort descending. The top-N items become the recommendation list, evaluated with NDCG@k and Precision@k.

---

## Key Design Decisions

| Aspect | Choice | Rationale |
|--------|--------|-----------|
| Algorithm | Funk SVD (from scratch) | Strong baseline, interpretable, fast on CPU |
| Framework | PyTorch | Full control over training loop, GPU-ready |
| Latent factors | 20 | Balanced expressiveness vs overfitting for 100k ratings |
| Optimiser | Adam (lr=0.005, L2=0.02) | Fast convergence, built-in weight decay |
| Relevance threshold | Rating ≥ 4.0 | Standard for converting explicit ratings to binary relevance |
| Train/test split | 80/20, seed=42 | Reproducible evaluation |
| Predictions | Clamped to [1, 5] | Keeps outputs on the valid MovieLens rating scale |

---

## Data

**MovieLens 100k** (GroupLens Research):
- 943 users, 1,682 movies, 100,000 ratings (1–5 scale)
- Located at `data/ml-100k/ml-100k/u.data`
- Tab-separated: `user_id  item_id  rating  timestamp`
- User demographics in `u.user`, movie metadata in `u.item`

---

## References

- [MovieLens 100k Dataset](https://grouplens.org/datasets/movielens/100k/) — GroupLens Research
- [Funk SVD](https://sifter.org/~simon/journal/20061211.html) — Simon Funk's Netflix Prize blog post
- [PyTorch](https://pytorch.org/) — model framework
